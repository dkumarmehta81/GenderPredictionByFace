{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7800340,"sourceType":"datasetVersion","datasetId":4567313},{"sourceId":200743,"sourceType":"datasetVersion","datasetId":87153}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"datasets = [\"/kaggle/input/cell-images-for-detecting-malaria/cell_images\"]\nclass_names_label = {'Parasitized': 0, 'Uninfected': 1}\nimport tensorflow as tf\nfrom skimage.color import rgb2gray\nimport os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nfrom skimage.feature import hog, local_binary_pattern\nfrom scipy.signal import convolve2d\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.mixed_precision import set_global_policy\nfrom tensorflow.keras.backend import clear_session\nimport random\n\n# Set mixed precision policy\nset_global_policy('mixed_float16')\n\n# Clear session\nclear_session()\nrandom.seed(42)  # Python random seed\nnp.random.seed(42)  # NumPy random seed\ntf.random.set_seed(42)  # TensorFlow random seed\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nfrom skimage.color import rgb2gray\nimport random\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n\n# Class mapping: Parasitized as 0, Uninfected as 1\nclass_names_label = {'Parasitized': 0, 'Uninfected': 1}\n\n# Preprocessing functions\ndef sharpen_image(image):\n    \"\"\"Apply sharpening to an image.\"\"\"\n    kernel = np.array([[0, -1, 0],\n                       [-1, 5, -1],\n                       [0, -1, 0]])\n    return cv2.filter2D(image, -1, kernel)\n\ndef gaussian_blur(image):\n    \"\"\"Apply Gaussian blur to smooth the image.\"\"\"\n    return cv2.GaussianBlur(image, (5, 5), 0)\n\ndef equalize_histogram(image):\n    \"\"\"Apply histogram equalization.\"\"\"\n    if len(image.shape) == 3:  # Color image\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        l = clahe.apply(l)\n        lab = cv2.merge((l, a, b))\n        return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n    else:  # Grayscale image\n        return cv2.equalizeHist(image)\n\ndef load_data_with_preprocessing(datasets, class_names_label):\n    images = []\n    labels = []\n    \n    for dataset in datasets:\n        print(\"Loading dataset:\", dataset)\n        for folder in os.listdir(dataset):\n            label = class_names_label.get(folder)\n            if label is None:\n                continue  # Skip folders not in the classification mapping\n            \n            folder_path = os.path.join(dataset, folder)\n            all_files = os.listdir(folder_path)\n            sampled_files = random.sample(all_files, len(all_files) // 6)  # Sample half\n            \n            for file in tqdm(sampled_files, desc=f\"Processing {folder}\"):\n                file_path = os.path.join(folder_path, file)\n                \n                # Load and preprocess image\n                image = cv2.imread(file_path)\n                if image is None:\n                    continue  # Skip corrupted images\n                \n                image = cv2.resize(image, (128, 128))  # Resize to 128x128\n                image = sharpen_image(image)          # Apply sharpening\n                image = gaussian_blur(image)         # Apply Gaussian blur\n                image = equalize_histogram(image)    # Apply histogram equalization\n                image = image / 255.0                # Normalize to [0, 1]\n                \n                images.append(image)\n                labels.append(label)\n    \n    images = np.array(images, dtype='float32')\n    labels = np.array(labels, dtype='int32')\n    return images, labels\n\n# Dataset path\ndatasets = [\"/kaggle/input/cell-images-for-detecting-malaria/cell_images\"]\n\n# Load data\nimages, labels = load_data_with_preprocessing(datasets, class_names_label)\n\n# Display dataset summary\nprint(\"Images shape:\", images.shape)\nprint(\"Labels shape:\", labels.shape)\nprint(\"Class distribution:\", dict(zip(*np.unique(labels, return_counts=True))))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom skimage.color import rgb2gray\n\ndef extract_hog_features(images):\n    hog_features = []\n    for image in images:\n        # Convert the image to grayscale\n        gray_image = rgb2gray(image)  # Converts RGB to grayscale\n        hog_feat = hog(gray_image, orientations=9, pixels_per_cell=(8, 8),\n                       cells_per_block=(2, 2), block_norm='L2-Hys', channel_axis=None)\n        hog_features.append(hog_feat)\n    return np.array(hog_features)\ndef extract_gabor_features(images):\n    gabor_features = []\n    num_kernels = 8  # Number of orientations\n    for image in images:\n        gray_image = rgb2gray(image)\n        feature_vector = []\n        for theta in range(num_kernels):\n            theta_rad = theta / num_kernels * np.pi\n            gabor_filter = create_gabor_filter(theta_rad)\n            filtered_image = convolve2d(gray_image, gabor_filter, mode='same')\n            feature_vector.append(np.mean(filtered_image))  # Use the mean as a feature\n        gabor_features.append(feature_vector)\n    return np.array(gabor_features)\n\n# Function to create Gabor filter\ndef create_gabor_filter(theta, sigma=1.0, frequency=0.5):\n    size = 21  # Kernel size\n    x, y = np.meshgrid(np.linspace(-1, 1, size), np.linspace(-1, 1, size))\n    rotx = x * np.cos(theta) + y * np.sin(theta)\n    roty = -x * np.sin(theta) + y * np.cos(theta)\n    gabor = np.exp(-(rotx**2 + roty**2) / (2 * sigma**2)) * np.cos(2 * np.pi * frequency * rotx)\n    return gabor\nfrom skimage.color import rgb2gray\nfrom skimage.feature import local_binary_pattern\nimport numpy as np\n\ndef extract_lbp_features(images, radius=3, n_points=24):\n    lbp_features = []\n    for image in images:\n        # Convert to grayscale\n        gray_image = rgb2gray(image)\n        \n        # Convert grayscale image to an integer type (e.g., uint8)\n        gray_image = (gray_image * 255).astype(np.uint8)  # Scaling to 255 and converting to uint8\n\n        # Compute LBP\n        lbp = local_binary_pattern(gray_image, n_points, radius, method=\"uniform\")\n\n        # Flatten the LBP histogram as a feature vector\n        (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n\n        # Normalize the histogram\n        hist = hist.astype(\"float\")\n        hist /= hist.sum()\n\n        lbp_features.append(hist)\n    \n    return np.array(lbp_features)\nimport cv2\nimport numpy as np\n\ndef extract_sift_features(images, max_features=128):\n    sift_features = []\n    sift = cv2.SIFT_create()\n    \n    for image in images:\n        gray_image = rgb2gray(image)  # Convert to grayscale\n        gray_image = (gray_image * 255).astype('uint8')  # Ensure correct format for SIFT\n        \n        keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n        \n        if descriptors is None:\n            descriptors = np.zeros((1, 128))  # Handle images with no keypoints\n        \n        # Pad or truncate to ensure uniform descriptor size\n        if descriptors.shape[0] > max_features:\n            descriptors = descriptors[:max_features]\n        elif descriptors.shape[0] < max_features:\n            padding = np.zeros((max_features - descriptors.shape[0], descriptors.shape[1]))\n            descriptors = np.vstack((descriptors, padding))\n        \n        # Flatten descriptors and add to the list\n        sift_features.append(descriptors.flatten())\n    \n    return np.array(sift_features)\n\ndef extract_contour_features(images, max_contours=10):\n    contour_features = []\n    \n    for image in images:\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n        _, binary = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)  # Apply binary thresholding\n        \n        # Ensure binary image is of type uint8 (CV_8UC1)\n        binary = binary.astype(np.uint8)\n        \n        # Find contours\n        contours, _ = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        \n        # Flatten the contours into a feature vector\n        contour_vector = np.zeros(max_contours, dtype=object)  # Initialize an array to store contour features\n        for i, contour in enumerate(contours[:max_contours]):\n            contour_vector[i] = contour.flatten()  # Flatten each contour\n        \n        contour_features.append(contour_vector.flatten())  # Flatten the entire contour list and append\n    \n    return np.array(contour_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n\n# Display the shape of the resulting datasets\nprint(\"Training data shape:\", X_train.shape)\nprint(\"Training labels shape:\", y_train.shape)\nprint(\"Testing data shape:\", X_test.shape)\nprint(\"Testing labels shape:\", y_test.shape)\n\n# Class distribution in training and testing sets\ntrain_distribution = dict(zip(*np.unique(y_train, return_counts=True)))\ntest_distribution = dict(zip(*np.unique(y_test, return_counts=True)))\nprint(\"Class distribution in training set:\", train_distribution)\nprint(\"Class distribution in testing set:\", test_distribution)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\ndef display_sample_images(images, labels, class_mapping, num_samples=5):\n    \"\"\"\n    Display sample images with their labels.\n    :param images: Array of image data\n    :param labels: Array of corresponding labels\n    :param class_mapping: Dictionary mapping class IDs to class names\n    :param num_samples: Number of samples to display\n    \"\"\"\n    plt.figure(figsize=(15, 5))\n    for i in range(num_samples):\n        idx = random.randint(0, len(images) - 1)\n        image = images[idx]\n        label = labels[idx]\n        class_name = class_mapping.get(label, \"Unknown\")  # Safely retrieve class name\n        \n        plt.subplot(1, num_samples, i + 1)\n        plt.imshow(image)\n        plt.title(f\"Class: {class_name}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Update class mapping\nclass_mapping = {0: 'Parasitized', 1: 'Uninfected'}\n\n# Display 5 random samples from the dataset\ndisplay_sample_images(images, labels, class_mapping, num_samples=5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature extraction\nhog_train = extract_hog_features(X_train)\nhog_test = extract_hog_features(X_test)\ngabor_train = extract_gabor_features(X_train)\ngabor_test = extract_gabor_features(X_test)\nlbp_train = extract_lbp_features(X_train)\nlbp_test = extract_lbp_features(X_test)\nsift_train = extract_sift_features(X_train)\nsift_test = extract_sift_features(X_test)\ncontour_train = extract_contour_features(X_train)\ncontour_test = extract_contour_features(X_test)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize features\nscaler = MinMaxScaler()\nsift_train = scaler.fit_transform(sift_train)\nsift_test = scaler.transform(sift_test)\nlbp_train = scaler.fit_transform(lbp_train)\nlbp_test = scaler.transform(lbp_test)\ncontour_train = scaler.fit_transform(contour_train)\ncontour_test = scaler.transform(contour_test)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine features\nfeature_inputs = {\n    'HOG': hog_train,\n    'Gabor': gabor_train,\n    'LBP': lbp_train,\n    'SIFT': sift_train,\n    'Contour': contour_train,\n}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, concatenate, GlobalAveragePooling2D, GlobalMaxPooling2D\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom itertools import combinations\nfrom tensorflow.keras.regularizers import l2\n\n# Input layer\ninput_image = Input(shape=(128, 128, 3), name='Image_Input')\n\n# Block 1\nconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_image)\n# b1 = BatchNormalization()(conv1)\npool1 = MaxPooling2D((2, 2))(conv1)\ndrop1 = Dropout(0.3)(pool1)\n\n# Block 2\nconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(drop1)\n# b2 = BatchNormalization()(conv2)\npool2 = MaxPooling2D((2, 2))(conv2)\ndrop2 = Dropout(0.4)(pool2)\n\n# Block 3\nconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(drop2)\n# b3 = BatchNormalization()(conv3)\npool3 = MaxPooling2D((2, 2))(conv3)\ndrop3 = Dropout(0.5)(pool3)\n\n# Block 4\nconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(drop3)\n# b4 = BatchNormalization()(conv4)\npool4 = MaxPooling2D((2, 2))(conv4)\ndrop4 = Dropout(0.6)(pool4)\n\n# # Block 4\n# conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(drop4)\n# # b4 = BatchNormalization()(conv4)\n# pool5 = MaxPooling2D((2, 2))(conv5)\n# drop5 = Dropout(0.5)(pool5)\n\n# Flatten the CNN output\nflat_cnn = Flatten()(drop4)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the feature inputs\ninput_hog = Input(shape=(hog_train.shape[1],), name='HOG_Input')\ninput_gabor = Input(shape=(gabor_train.shape[1],), name='Gabor_Input')\ninput_lbp = Input(shape=(lbp_train.shape[1],), name='LBP_Input')\n# Add SIFT input\ninput_sift = Input(shape=(sift_train.shape[1],), name='SIFT_Input')\n#Add Contour\ninput_contour = Input(shape=(contour_train.shape[1],), name='Contour_Input')\n#Add_KAZE\n# input_qmft=Input(shape=(qmft_train.shape[1],), name='QMFT_Input')\n\n# Define all feature inputs\nfeature_inputs = {\n    'CNN': flat_cnn,\n    'HOG': input_hog,\n    'Gabor': input_gabor,\n    'LBP': input_lbp,\n    'SIFT': input_sift,\n    'Contour':input_contour,\n    # 'QMFT':input_qmft\n}\n\n# Define all input shapes\ninput_shapes = {\n    'CNN': input_image,\n    'HOG': input_hog,\n    'Gabor': input_gabor,\n    'LBP': input_lbp,\n    'SIFT': input_sift,\n    'Contour':input_contour,\n    # 'QMFT':input_qmft\n}\n\n# Test all combinations of features\nresults = []\nfor r in range(1, len(feature_inputs) + 1):\n    for selected_features in combinations(feature_inputs.keys(), r):\n        print(f\"Testing combination: {selected_features}\")\n        \n        # Combine selected features\n        combined_inputs = [feature_inputs[feat] for feat in selected_features]\n        combined_layer = concatenate(combined_inputs)\n        \n        # Fully connected layers\n        dense1 = Dense(512, activation='relu')(combined_layer)\n        drop4 = Dropout(0.5)(dense1)\n        # dense2 = Dense(256, activation='relu')(drop4)\n        # drop5 = Dropout(0.3)(dense2)\n        finalout = Dense(3, activation='softmax')(drop4)\n        \n        # Build model with selected inputs\n        selected_inputs = [input_shapes[feat] for feat in selected_features]\n        model = Model(inputs=selected_inputs, outputs=finalout)\n        model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        \n        # Prepare inputs for training\n        train_inputs = [X_train if feat == 'CNN' else eval(f\"{feat.lower()}_train\") for feat in selected_features]\n        test_inputs = [X_test if feat == 'CNN' else eval(f\"{feat.lower()}_test\") for feat in selected_features]\n        \n        # Callbacks for early stopping and model checkpointing\n        model_name = f\"model_combination_{'_'.join(selected_features)}.keras\"\n        early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1)\n        model_checkpoint = ModelCheckpoint(model_name, monitor='val_accuracy', save_best_only=True, verbose=1)\n        \n        # Train the model\n        history = model.fit(\n            train_inputs, \n            y_train, \n            batch_size=32, \n            epochs=20, \n            validation_split=0.2,\n            shuffle=True,  # Set shuffle to False\n            callbacks=[early_stop, model_checkpoint],\n            verbose=1\n        )\n        \n        # Evaluate the model\n        test_loss, test_acc = model.evaluate(test_inputs, y_test, verbose=0)\n        results.append((selected_features, test_acc))\n        print(f\"Combination {selected_features} -> Test Accuracy: {test_acc:.4f}\")\n        \n# Print all results\nprint(\"\\nSummary of Results:\")\nfor comb, acc in results:\n    print(f\"Features: {comb}, Accuracy: {acc:.4f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sort results by accuracy in descending order\nsorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Print all results\nprint(\"\\nSummary of Results (Sorted by Accuracy):\")\nfor comb, acc in sorted_results:\n    print(f\"Features: {comb}, Accuracy: {acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nmodel = load_model('model_combination_CNN_HOG_Gabor_LBP.keras')\ntest_inputs = [X_test,hog_test,gabor_test,lbp_test]  # Ensure these variables are defined\ny_pred_probs = model.predict(test_inputs)\ny_pred = np.argmax(y_pred_probs, axis=1)  # Get the predicted class labels\n\n# Generate the classification report\nreport = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\nprint(report)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    conf_matrix,\n    annot=True,\n    fmt='d',\n    cmap='cubehelix',\n    xticklabels=['Parasitized', 'Normal'],\n    yticklabels=['Parasitized', 'Normal'],\n    annot_kws={\"size\": 12, \"weight\": \"bold\"}\n)\n\n# Add bold text to titles, labels, and ticks\nplt.title(\"CNN+HOG+Gabor+LBP Malaria Cell CM\", fontweight='bold', fontsize=16)\nplt.xlabel(\"Predicted Label\", fontweight='bold', fontsize=14)\nplt.ylabel(\"True Label\", fontweight='bold', fontsize=14)\n\nplt.xticks(fontweight='bold', fontsize=12)\nplt.yticks(fontweight='bold', fontsize=12)\n\n# Save the figure with high DPI\nplt.savefig(\"Malaria Cell CNN_HOG_Gabor_LBPCM.png\", dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model('model_combination_CNN_HOG_Contour.keras')\ntest_inputs = [X_test,hog_test,contour_test]  # Ensure these variables are defined\ny_pred_probs = model.predict(test_inputs)\ny_pred = np.argmax(y_pred_probs, axis=1)  # Get the predicted class labels\n\n# Generate the classification report\nreport = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\nprint(report)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    conf_matrix,\n    annot=True,\n    fmt='d',\n    cmap='YlGnBu',\n    xticklabels=['Parasitized', 'Normal'],\n    yticklabels=['Parasitized', 'Normal'],\n    annot_kws={\"size\": 12, \"weight\": \"bold\"}\n)\n\n# Add bold text to titles, labels, and ticks\nplt.title(\"CNN+HOG+Contour Malaria Cell CM\", fontweight='bold', fontsize=16)\nplt.xlabel(\"Predicted Label\", fontweight='bold', fontsize=14)\nplt.ylabel(\"True Label\", fontweight='bold', fontsize=14)\n\nplt.xticks(fontweight='bold', fontsize=12)\nplt.yticks(fontweight='bold', fontsize=12)\n\n# Save the figure with high DPI\nplt.savefig(\"Malaria Cell CNN_HOG_ContourCM.png\", dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model('model_combination_CNN_HOG_Gabor_LBP_SIFT.keras')\ntest_inputs = [X_test,hog_test,gabor_test,lbp_test,sift_test]  # Ensure these variables are defined\n# Predict class probabilities and class labels\ny_pred_probs = model.predict(test_inputs)\ny_pred = np.argmax(y_pred_probs, axis=1)  # Get the predicted class labels\n\n# Generate the classification report\nreport = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])\nprint(report)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    conf_matrix,\n    annot=True,\n    fmt='d',\n    cmap='BuPu',\n    xticklabels=['Parasitized', 'Normal'],\n    yticklabels=['Parasitized', 'Normal'],\n    annot_kws={\"size\": 12, \"weight\": \"bold\"}\n)\n\n# Add bold text to titles, labels, and ticks\nplt.title(\"CNN+HOG+Gabor+LBP+SIFT Malaria Cell CM\", fontweight='bold', fontsize=16)\nplt.xlabel(\"Predicted Label\", fontweight='bold', fontsize=14)\nplt.ylabel(\"True Label\", fontweight='bold', fontsize=14)\n\nplt.xticks(fontweight='bold', fontsize=12)\nplt.yticks(fontweight='bold', fontsize=12)\n\n# Save the figure with high DPI\nplt.savefig(\"Malaria Cell CNN_HOG_Gabor_LBP_SIFTCM.png\", dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom skimage.feature import hog, local_binary_pattern\nfrom skimage.filters import gabor\nfrom skimage.color import rgb2gray\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.mixed_precision import set_global_policy\nfrom tensorflow.keras.backend import clear_session\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom scipy.signal import convolve2d\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\nfrom itertools import combinations\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\n\n# Set mixed precision policy\nset_global_policy('mixed_float16')\n\n# Clear session\nclear_session()\nrandom.seed(42)  # Python random seed\nnp.random.seed(42)  # NumPy random seed\ntf.random.set_seed(42)  # TensorFlow random seed\n\n# Dataset path and class names\ndatasets = [\"/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image\"]\nclass_names_label = {'Lung_Opacity': 0, 'Normal': 1, 'Viral Pneumonia': 2}\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing functions\ndef sharpen_image(image):\n    \"\"\"Apply sharpening to an image.\"\"\"\n    kernel = np.array([[0, -1, 0],\n                       [-1, 5, -1],\n                       [0, -1, 0]])\n    sharpened = cv2.filter2D(image, -1, kernel)\n    return sharpened\n\ndef equalize_histogram(image):\n    \"\"\"Apply histogram equalization.\"\"\"\n    if len(image.shape) == 3:  # Color image\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        l = clahe.apply(l)\n        lab = cv2.merge((l, a, b))\n        equalized_image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n    else:  # Grayscale image\n        equalized_image = cv2.equalizeHist(image)\n    return equalized_image\n\ndef gaussian_blur(image):\n    \"\"\"Apply Gaussian blur to smooth the image.\"\"\"\n    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n    return blurred\n\n# Feature extraction functions\ndef extract_hog_features(images):\n    hog_features = []\n    for image in images:\n        gray_image = rgb2gray(image)  # Converts RGB to grayscale\n        hog_feat = hog(gray_image, orientations=9, pixels_per_cell=(8, 8),\n                       cells_per_block=(2, 2), block_norm='L2-Hys', channel_axis=None)\n        hog_features.append(hog_feat)\n    return np.array(hog_features)\n\ndef extract_gabor_features(images):\n    gabor_features = []\n    num_kernels = 8  # Number of orientations\n    for image in images:\n        gray_image = rgb2gray(image)\n        feature_vector = []\n        for theta in range(num_kernels):\n            theta_rad = theta / num_kernels * np.pi\n            gabor_filter = create_gabor_filter(theta_rad)\n            filtered_image = convolve2d(gray_image, gabor_filter, mode='same')\n            feature_vector.append(np.mean(filtered_image))  # Use the mean as a feature\n        gabor_features.append(feature_vector)\n    return np.array(gabor_features)\n\ndef create_gabor_filter(theta, sigma=1.0, frequency=0.5):\n    size = 21  # Kernel size\n    x, y = np.meshgrid(np.linspace(-1, 1, size), np.linspace(-1, 1, size))\n    rotx = x * np.cos(theta) + y * np.sin(theta)\n    roty = -x * np.sin(theta) + y * np.cos(theta)\n    gabor = np.exp(-(rotx**2 + roty**2) / (2 * sigma**2)) * np.cos(2 * np.pi * frequency * rotx)\n    return gabor\n\ndef extract_lbp_features(images, radius=3, n_points=24):\n    lbp_features = []\n    for image in images:\n        gray_image = rgb2gray(image)\n        gray_image = (gray_image * 255).astype(np.uint8)  # Scaling to 255 and converting to uint8\n        lbp = local_binary_pattern(gray_image, n_points, radius, method=\"uniform\")\n        (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n        hist = hist.astype(\"float\")\n        hist /= hist.sum()\n        lbp_features.append(hist)\n    return np.array(lbp_features)\n\ndef extract_sift_features(images, max_features=128):\n    sift_features = []\n    sift = cv2.SIFT_create()\n    for image in images:\n        gray_image = rgb2gray(image)\n        gray_image = (gray_image * 255).astype('uint8')\n        keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n        if descriptors is None:\n            descriptors = np.zeros((1, 128))\n        if descriptors.shape[0] > max_features:\n            descriptors = descriptors[:max_features]\n        elif descriptors.shape[0] < max_features:\n            padding = np.zeros((max_features - descriptors.shape[0], descriptors.shape[1]))\n            descriptors = np.vstack((descriptors, padding))\n        sift_features.append(descriptors.flatten())\n    return np.array(sift_features)\n\ndef extract_contour_features(images, max_contours=10):\n    contour_features = []\n    for image in images:\n        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        _, binary = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n        binary = binary.astype(np.uint8)\n        contours, _ = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        contour_vector = np.zeros(max_contours, dtype=object)\n        for i, contour in enumerate(contours[:max_contours]):\n            contour_vector[i] = contour.flatten()\n        contour_features.append(contour_vector.flatten())\n    return np.array(contour_features)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nfrom skimage.color import rgb2gray\nimport random\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\n\n# Class mapping: Parasitized as 0, Uninfected as 1\nclass_names_label = {'Parasitized': 0, 'Uninfected': 1}\n\n# Preprocessing functions\ndef sharpen_image(image):\n    \"\"\"Apply sharpening to an image.\"\"\"\n    kernel = np.array([[0, -1, 0],\n                       [-1, 5, -1],\n                       [0, -1, 0]])\n    return cv2.filter2D(image, -1, kernel)\n\ndef gaussian_blur(image):\n    \"\"\"Apply Gaussian blur to smooth the image.\"\"\"\n    return cv2.GaussianBlur(image, (5, 5), 0)\n\ndef equalize_histogram(image):\n    \"\"\"Apply histogram equalization.\"\"\"\n    if len(image.shape) == 3:  # Color image\n        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        l = clahe.apply(l)\n        lab = cv2.merge((l, a, b))\n        return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n    else:  # Grayscale image\n        return cv2.equalizeHist(image)\n\ndef load_data_with_preprocessing(datasets, class_names_label):\n    images = []\n    labels = []\n    \n    for dataset in datasets:\n        print(\"Loading dataset:\", dataset)\n        for folder in os.listdir(dataset):\n            label = class_names_label.get(folder)\n            if label is None:\n                continue  # Skip folders not in the classification mapping\n            \n            folder_path = os.path.join(dataset, folder)\n            all_files = os.listdir(folder_path)\n            sampled_files = random.sample(all_files, len(all_files) // 6)  # Sample half\n            \n            for file in tqdm(sampled_files, desc=f\"Processing {folder}\"):\n                file_path = os.path.join(folder_path, file)\n                \n                # Load and preprocess image\n                image = cv2.imread(file_path)\n                if image is None:\n                    continue  # Skip corrupted images\n                \n                image = cv2.resize(image, (128, 128))  # Resize to 128x128\n                image = sharpen_image(image)          # Apply sharpening\n                image = gaussian_blur(image)         # Apply Gaussian blur\n                image = equalize_histogram(image)    # Apply histogram equalization\n                image = image / 255.0                # Normalize to [0, 1]\n                \n                images.append(image)\n                labels.append(label)\n    \n    images = np.array(images, dtype='float32')\n    labels = np.array(labels, dtype='int32')\n    return images, labels\n\n# Dataset path\ndatasets = [\"/kaggle/input/cell-images-for-detecting-malaria/cell_images\"]\n\n# Load data\nimages, labels = load_data_with_preprocessing(datasets, class_names_label)\n\n# Display dataset summary\nprint(\"Images shape:\", images.shape)\nprint(\"Labels shape:\", labels.shape)\nprint(\"Class distribution:\", dict(zip(*np.unique(labels, return_counts=True))))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load data with preprocessing\n# def load_data_with_preprocessing(datasets, class_names_label):\n#     images = []\n#     labels = []\n#     for dataset in datasets:\n#         print(\"Loading {}\".format(dataset))\n#         for folder in os.listdir(dataset):\n#             label = class_names_label[folder]\n#             for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n#                 img_path = os.path.join(os.path.join(dataset, folder), file)\n#                 image = cv2.imread(img_path)\n#                 image = cv2.resize(image, (128, 128))\n#                 image = sharpen_image(image)\n#                 image = gaussian_blur(image)\n#                 image = equalize_histogram(image)\n#                 image = image / 255.0\n#                 images.append(image)\n#                 labels.append(label)\n#     images = np.array(images, dtype='float32')\n#     labels = np.array(labels, dtype='int32')\n#     return images, labels\n\n# # Load data\n# images, labels = load_data_with_preprocessing(datasets, class_names_label)\n# print(\"Images shape:\", images.shape)\n# print(\"Labels shape:\", labels.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\nprint(\"Training data shape:\", X_train.shape)\nprint(\"Testing data shape:\", X_test.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract features\nhog_train = extract_hog_features(X_train)\nhog_test = extract_hog_features(X_test)\ngabor_train = extract_gabor_features(X_train)\ngabor_test = extract_gabor_features(X_test)\nlbp_train = extract_lbp_features(X_train)\nlbp_test = extract_lbp_features(X_test)\nsift_train = extract_sift_features(X_train)\nsift_test = extract_sift_features(X_test)\ncontour_train = extract_contour_features(X_train)\ncontour_test = extract_contour_features(X_test)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize features\nscaler = MinMaxScaler()\nhog_train = scaler.fit_transform(hog_train)\nhog_test = scaler.transform(hog_test)\ngabor_train = scaler.fit_transform(gabor_train)\ngabor_test = scaler.transform(gabor_test)\nlbp_train = scaler.fit_transform(lbp_train)\nlbp_test = scaler.transform(lbp_test)\nsift_train = scaler.fit_transform(sift_train)\nsift_test = scaler.transform(sift_test)\ncontour_train = scaler.fit_transform(contour_train)\ncontour_test = scaler.transform(contour_test)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom skimage.feature import hog, local_binary_pattern\nfrom skimage.filters import gabor\nfrom skimage.color import rgb2gray\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.mixed_precision import set_global_policy\nfrom tensorflow.keras.backend import clear_session\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom scipy.signal import convolve2d\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\nfrom itertools import combinations\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\n\n# Set mixed precision policy\nset_global_policy('mixed_float16')\n\n# Clear session\nclear_session()\nrandom.seed(42)  # Python random seed\nnp.random.seed(42)  # NumPy random seed\ntf.random.set_seed(42)  # TensorFlow random seed\n\n# Dataset path and class names\ndatasets = [\"/kaggle/input/lung-disease/Lung X-Ray Image/Lung X-Ray Image\"]\nclass_names_label = {'Lung_Opacity': 0, 'Normal': 1, 'Viral Pneumonia': 2}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define CNN model\ninput_image = Input(shape=(128, 128, 3), name='Image_Input')\nconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_image)\npool1 = MaxPooling2D((2, 2))(conv1)\ndrop1 = Dropout(0.3)(pool1)\nconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(drop1)\npool2 = MaxPooling2D((2, 2))(conv2)\ndrop2 = Dropout(0.4)(pool2)\nconv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(drop2)\npool3 = MaxPooling2D((2, 2))(conv3)\ndrop3 = Dropout(0.5)(pool3)\nconv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(drop3)\npool4 = MaxPooling2D((2, 2))(conv4)\ndrop4 = Dropout(0.6)(pool4)\nflat_cnn = Flatten()(drop4)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define feature inputs\ninput_hog = Input(shape=(hog_train.shape[1],), name='HOG_Input')\ninput_gabor = Input(shape=(gabor_train.shape[1],), name='Gabor_Input')\ninput_lbp = Input(shape=(lbp_train.shape[1],), name='LBP_Input')\ninput_sift = Input(shape=(sift_train.shape[1],), name='SIFT_Input')\ninput_contour = Input(shape=(contour_train.shape[1],), name='Contour_Input')\n\n# Feature inputs dictionary\nfeature_inputs = {\n    'CNN': flat_cnn,\n    'HOG': input_hog,\n    'Gabor': input_gabor,\n    'LBP': input_lbp,\n    'SIFT': input_sift,\n    'Contour': input_contour\n}\n\n# Input shapes dictionary\ninput_shapes = {\n    'CNN': input_image,\n    'HOG': input_hog,\n    'Gabor': input_gabor,\n    'LBP': input_lbp,\n    'SIFT': input_sift,\n    'Contour': input_contour\n}\n\n# Train and evaluate models\nresults = []\nfor r in range(1, len(feature_inputs) + 1):\n    for selected_features in combinations(feature_inputs.keys(), r):\n        print(f\"Testing combination: {selected_features}\")\n        combined_inputs = [feature_inputs[feat] for feat in selected_features]\n        combined_layer = concatenate(combined_inputs)\n        dense1 = Dense(512, activation='relu')(combined_layer)\n        drop4 = Dropout(0.5)(dense1)\n        finalout = Dense(2, activation='sigmoid')(drop4)\n        selected_inputs = [input_shapes[feat] for feat in selected_features]\n        model = Model(inputs=selected_inputs, outputs=finalout)\n        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n        train_inputs = [X_train if feat == 'CNN' else eval(f\"{feat.lower()}_train\") for feat in selected_features]\n        test_inputs = [X_test if feat == 'CNN' else eval(f\"{feat.lower()}_test\") for feat in selected_features]\n        model_name = f\"model_combination_{'_'.join(selected_features)}.keras\"\n        early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1)\n        model_checkpoint = ModelCheckpoint(model_name, monitor='val_accuracy', save_best_only=True, verbose=1)\n        history = model.fit(\n            train_inputs, \n            y_train, \n            batch_size=16, \n            epochs=20, \n            validation_split=0.2,\n            shuffle=True,\n            callbacks=[early_stop, model_checkpoint],\n            verbose=1\n        )\n        test_loss, test_acc = model.evaluate(test_inputs, y_test, verbose=0)\n        results.append((selected_features, test_acc))\n        print(f\"Combination {selected_features} -> Test Accuracy: {test_acc:.4f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sort results by accuracy\nsorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n\n# Save top 3 models\nfor i, (selected_features, acc) in enumerate(sorted_results[:3]):\n    model_name = f\"top_{i+1}_model_{'_'.join(selected_features)}.keras\"\n    print(f\"Saving {model_name} with accuracy: {acc:.4f}\")\n    combined_inputs = [feature_inputs[feat] for feat in selected_features]\n    combined_layer = concatenate(combined_inputs)\n    dense1 = Dense(512, activation='relu')(combined_layer)\n    drop4 = Dropout(0.5)(dense1)\n    finalout = Dense(3, activation='softmax')(drop4)\n    selected_inputs = [input_shapes[feat] for feat in selected_features]\n    model = Model(inputs=selected_inputs, outputs=finalout)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    train_inputs = [X_train if feat == 'CNN' else eval(f\"{feat.lower()}_train\") for feat in selected_features]\n    model.fit(\n        train_inputs, \n        y_train, \n        batch_size=16, \n        epochs=20, \n        validation_split=0.2,\n        shuffle=True,\n        callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1)],\n        verbose=1\n    )\n    model.save(model_name)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Preprocessing function\ndef preprocess_image(image):\n    \"\"\"\n    Apply preprocessing steps to an image:\n    1. Sharpening\n    2. Gaussian blur\n    3. Histogram equalization\n    \"\"\"\n    # Ensure the image is in the correct format (uint8)\n    if image.dtype != np.uint8:\n        image = (image * 255).astype(np.uint8)  # Scale to [0, 255] and convert to uint8\n    \n    # Sharpening\n    image = sharpen_image(image)\n    \n    # Gaussian blur\n    image = gaussian_blur(image)\n    \n    # Histogram equalization\n    image = equalize_histogram(image)\n    \n    return image\n\n# Ablation study on the best model\nbest_model_features = sorted_results[0][0]\nprint(f\"\\nPerforming ablation study on the best model with features: {best_model_features}\")\nablation_results = []\n\nfor feature in best_model_features:\n    ablated_features = list(best_model_features)\n    ablated_features.remove(feature)\n    print(f\"\\nAblating feature: {feature}\")\n    \n    # Rebuild the model with ablated features\n    combined_inputs = [feature_inputs[feat] for feat in ablated_features]\n    combined_layer = concatenate(combined_inputs)\n    dense1 = Dense(512, activation='relu')(combined_layer)\n    drop4 = Dropout(0.5)(dense1)\n    finalout = Dense(3, activation='softmax')(drop4)\n    selected_inputs = [input_shapes[feat] for feat in ablated_features]\n    model = Model(inputs=selected_inputs, outputs=finalout)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    # Prepare train and test inputs with preprocessing\n    train_inputs = []\n    test_inputs = []\n    \n    for feat in ablated_features:\n        if feat == 'CNN':\n            # Preprocess images for CNN\n            X_train_preprocessed = np.array([preprocess_image(img) for img in X_train])\n            X_test_preprocessed = np.array([preprocess_image(img) for img in X_test])\n            train_inputs.append(X_train_preprocessed)\n            test_inputs.append(X_test_preprocessed)\n        else:\n            # Use pre-extracted features for other feature types\n            train_inputs.append(eval(f\"{feat.lower()}_train\"))\n            test_inputs.append(eval(f\"{feat.lower()}_test\"))\n    \n    # Train the model\n    model.fit(\n        train_inputs, \n        y_train, \n        batch_size=16, \n        epochs=20, \n        validation_split=0.2,\n        shuffle=True,\n        callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1)],\n        verbose=1\n    )\n    \n    # Evaluate the model\n    test_loss, test_acc = model.evaluate(test_inputs, y_test, verbose=0)\n    ablation_results.append((feature, test_acc))\n    \n    # Generate predictions for the classification report\n    y_pred = model.predict(test_inputs)\n    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred_classes, target_names=class_names_label.keys()))\n    \n    # Print ablation results\n    print(f\"Ablated Features: {ablated_features}, Test Accuracy: {test_acc:.4f}\")\n\n# Print ablation study summary\nprint(\"\\nAblation Study Summary:\")\nfor feature, acc in ablation_results:\n    print(f\"Removed Feature: {feature}, Accuracy: {acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\n# Create a directory to save confusion matrices\noutput_dir = \"confusion_matrices\"\nos.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n\n# List of colormaps to use for each confusion matrix\ncolormaps = ['YlOrBr', 'Greens', 'Reds']\n\n# Function to evaluate the model and generate classification report and confusion matrix\ndef evaluate_model(model, test_inputs, y_test, selected_features, cmap, model_index):\n    # Predict on the test set\n    y_pred = model.predict(test_inputs)\n    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n\n    # Classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred_classes, target_names=class_names_label.keys()))\n\n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred_classes)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(\n        cm, \n        annot=True, \n        fmt='d', \n        cmap=cmap, \n        xticklabels=class_names_label.keys(), \n        yticklabels=class_names_label.keys(),\n        annot_kws={'size': 12, 'weight': 'bold'}  # Increase font size and make it bold\n    )\n    plt.title(f\"Confusion Matrix for Model with Features: {selected_features}\", fontweight='bold', fontsize=12)\n    plt.xticks(fontweight='bold', fontsize=12)\n    plt.yticks(fontweight='bold', fontsize=12)\n    plt.xlabel(\"Predicted\", fontweight='bold', fontsize=12)\n    plt.ylabel(\"True\", fontweight='bold', fontsize=12)\n    plt.tight_layout()\n    \n    # Convert selected_features tuple to a valid filename string\n    filename = \"_\".join(selected_features) + \".png\"  # Example: \"CNN_HOG.png\"\n    output_path = os.path.join(output_dir, filename)  # Save in the output directory\n    \n    # Save the confusion matrix as an image\n    plt.savefig(output_path, bbox_inches='tight', dpi=300)  # Save with high resolution\n    plt.show()  # Display the plot\n    plt.close()  # Close the plot to free up memory\n\n# Evaluate the top 3 models\nfor i, (selected_features, acc) in enumerate(sorted_results[:3]):\n    print(f\"\\nEvaluating Top {i+1} Model with Features: {selected_features}\")\n    \n    # Rebuild the model\n    combined_inputs = [feature_inputs[feat] for feat in selected_features]\n    combined_layer = concatenate(combined_inputs)\n    dense1 = Dense(512, activation='relu')(combined_layer)\n    drop4 = Dropout(0.5)(dense1)\n    finalout = Dense(3, activation='softmax')(drop4)\n    selected_inputs = [input_shapes[feat] for feat in selected_features]\n    model = Model(inputs=selected_inputs, outputs=finalout)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    # Load the saved model weights\n    model_name = f\"top_{i+1}_model_{'_'.join(selected_features)}.keras\"\n    model.load_weights(model_name)\n    \n    # Prepare test inputs\n    test_inputs = [X_test if feat == 'CNN' else eval(f\"{feat.lower()}_test\") for feat in selected_features]\n    \n    # Evaluate the model with a unique colormap\n    evaluate_model(model, test_inputs, y_test, selected_features, cmap=colormaps[i], model_index=i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}